We will start this chapter with general considerations about systems of partial differential equations (basic definitions, existence theorem and the notion of characteristics). More specifically, it will be seen that partial differential equations of high order can be reduced to first order systems of partial differential equations, in which fall equations of dynamics in solid mechanics (see section \ref{sec:solidMech_equations}). Then, the general concepts introduced in section \ref{sec:PDEs} will be applied in the context of solid mechanics conservation laws (section \ref{sec:characteristic_analysis}) in order to derive analytical solutions of one-dimensional problems in section \ref{sec:analytical_results}.
\subsection{Generalities}

A \textit{system of partial differential equations} can be written by means of an operator $F$ depending on sets of independent variables $x_1,...,x_N$ and dependent variables $u_1,...,u_I$:
\begin{equation}
  \label{eq:diff_operator}
  \vect{\Fc}\(x_1,...,x_N,u_1,...,u_I,\drond{u_1}{x_1},..., \drond{^Mu_I}{x_N^M}\) = \vect{0}
\end{equation}
The dimension of the system given by the operator \eqref{eq:diff_operator} is given by the size of the array $\vect{\Uc}^T=[u_1,...,u_I] \in \Rbb^I$, also referred to as the \textit{unknown vector}. The highest derivative of the unknown vector in the system defines the \textit{order of the system}. 

\paragraph{Notations:}In equation \eqref{eq:diff_operator} as in what follows, sans-serif symbols will refer to matrices while calligraphic symbols stand for column array. Furthermore, the partial derivative of an arbitrary quantity with respect to a given variable $\drond{u}{x}$ may be replaced by $u_x$ when there will be no ambiguity.

$\newline$
Making use of index notation and the convention of implicit summation over repeated indices, a system of partial differential equations can be written:
\begin{equation*}
  \sum_{k=1}^{N}\Asf_{ij}^p \drond{^p\Uc_j}{x_k^p} + \Bc_i = 0
\end{equation*}
or equivalently, in matrix form:
\begin{equation}
  \label{eq:diff_system_matrix}
  \sum_{k=1}^{N}\tens{\Asf}^p \drond{^p\vect{\Uc}}{x_k^p} + \vect{\Bc} =  \vect{0}
\end{equation}
The coefficients matrices $\tens{\Asf}^p$ and the vector $\vect{\Bc}$ may be functions of independent variables and the unknowns vector ($x_1,...,x_N,\vect{\Uc}$) leading to different types of partial differential systems. Namely, whether those terms are functions of the $x_k$ are not leads respectively to a \textit{linear system with variable coefficients} or to a \textit{linear system with constant coefficients}. The system remains \textit{linear} if $\vect{\Bc}$ depends linearly on $\vect{\Uc}$, and is \textit{semi-linear} if the relation is non-linear. Finally, if $\tens{\Asf}^p$ depends on independent variables, $\vect{\Uc}$ and its derivatives up to order $M-1$, the system is called \textit{quasi-linear}.

Solving the system of partial differential equations means that one seeks an unknown vector $\vect{\Uc}(x_1,...,x_N)$ that satisfies equations of the type of \eqref{eq:diff_system_matrix}. Geometrically speaking, the problem of finding the $I$ components of $\vect{\Uc}$ as functions of $N$ independent variables can be seen as the building of an hyper-surface surface of $\Rbb^{I+N}$, hence the term of \textit{integral surface} for $\vect{\Uc}$. Similarly to ordinary differential equations for which a one-parameter family of solutions depending on integration constants can reduce to one unique solution providing suitable conditions are used, the solution of PDEs involves arbitrary functions that can be set so that unicity is ensured. This is the object of what follows.

\paragraph{The theorem of Cauchy-Kowalevski:}
We consider here a partial differential system of dimension $I$ and order $M$ and of $N$ independent variables $x_k$. It is assumed that those system can be written:
\begin{equation}
  \label{eq:normal_form}
  \drond{^M\Uc_i}{x_1^M} = \Fc_i\(x_1,...,x_N,\Uc_1,...,\Uc_I,\drond{\Uc_1}{x_1},..., \drond{^M\Uc_I}{x_N^M} \)
\end{equation}
where the functions $\Fc_i$ do not depend on the left-hand side. In addition, initial conditions are prescribed for the derivatives of $\Uc_i$ with respect to $x_1$ up to order $M-1$ in the plane $x_1=0$, namely:
\begin{equation}
  \label{eq:IC_Cauchy}
  \begin{aligned}
    & \Uc_i(0,x_2,...,x_N) = \Gc_i(x_2,...,x_N) \\
    & ... \\
    & \drond{^{M-1}}{x_1^{M-1}}\Uc_i(0,x_2,...,x_N) = \Gc_i^{M-1}(x_2,...,x_N) 
  \end{aligned}
\end{equation}
where the $\Gc_i^p$ are arbitrary functions defined in the vicinity of the origin $(x_k=0 \:, \: k\ne 1)$.
% where $\Fc_i$ are functions that depend analytically on its variables (i.e. they can be expanded into power series in all these variables converging in a suitably small domain, which may be assumed to contain the origin x_k=0 \Uc_i=0)
The \textit{initial value problem} or \textit{Cauchy problem} consists in constructing a solution $\vect{\Uc}$ of system \eqref{eq:normal_form} that satisfies initial conditions \eqref{eq:IC_Cauchy}. The \textit{Cauchy--Kowalevski theorem} states that if functions $\Fc_i$ and $\Gc_i^p$ are analytic in the vicinity of the origin $x_k=0$, then Cauchy problem admits one and only one solution in the neighborhood of the origin (see \cite[Chapter~1]{Courant} for a proof of the theorem). % +Holmgren theorem

\paragraph{Reduction of high order PDEs to first order quasi-linear systems:} The Cauchy problem for a single partial differential equation of order $N$ can be reduced to an initial value problem for a first order quasi-linear system by means of a suitable change of variable. To illustrate this property, let us considers the following second order partial differential equation of two independent variables $(x,y)$ for the unknown $u$ (the result being extendable to more variables):
\begin{equation}
  \label{eq:2nd_order_pde}
  F(x,y,u,u_x,u_y,u_{xx},u_{yy},u_{xy})=0
\end{equation}
It is assumed that is equation can be rewritten as:
\begin{equation}
  \label{eq:2nd_order_pde_normal}
  u_{xx}= f(x,y,u,u_x,u_y,u_{yy},u_{xy})
\end{equation}
with associated initial conditions:
\begin{equation}
  \label{eq:2nd_order_pde_ICs}
  u(0,y)= g(y) \quad ; \quad u_x(0,y)= g_1(y)
\end{equation}
Then, applying the following change of variables:
\begin{equation*}
  \label{eq:change_of_variables}
  \begin{aligned}
     p &= u_x\\
     q &= u_{y} \\
     r &= u_{xx} \\
     s &= u_{yy}\\
     t &= u_{xy}
  \end{aligned}
\end{equation*}
and differentiating equation \eqref{eq:2nd_order_pde_normal} with respect to $x$:
\begin{equation*}
  \label{eq:r_x}
  u_{xxx}= f_x + f_u u_x + f_{u_x}u_{xx} + f_{u_y}u_{yx} + f_{u_{yy}}u_{yyx}+f_{u_{yx}}u_{yxx}
\end{equation*}
yields the first order partial differential system:
\begin{equation}
  \label{eq:1st_order_quasilinear_system}
  \begin{aligned}
    u_x & = p \\
    q_x & = p_y \\
    p_x & = r \\
    t_x & = r_y \\
    s_x & = t_y \\
    r_x & = f_x + f_up + f_p r + f_q p_y + f_s t_y + f_t r_y
  \end{aligned}
\end{equation}
Furthermore, from \eqref{eq:2nd_order_pde_ICs} and equation \eqref{eq:2nd_order_pde_normal}, initial conditions associated to system \eqref{eq:1st_order_quasilinear_system} can be determined:
\begin{equation}
  \label{eq:1st_order_quasilinear_system_ICs}
  \begin{aligned}
    u(0,y) &= g(y)\\
    p(0,y) &= g_1(y) \\
    q(0,y) &= g'(y) \\
    s(0,y) &= g''(y) \\
    t(0,y) &= g_1'(y) \\
    r(0,y) &= f(0,y,g(y),g_1(y),g'(y),g''(y),g_1'(y))
  \end{aligned}
\end{equation}
The solution of problem \eqref{eq:1st_order_quasilinear_system}--\eqref{eq:1st_order_quasilinear_system_ICs}  is equivalent to that of equation \eqref{eq:2nd_order_pde} \cite{Courant}.
As a consequence, in what follows we will focus on first order quasi-linear systems of partial differential equations depending on two independent variables $(t,x)$.
\subsection{The notion of characteristics -- Hyperbolicity}
The previous discussions are based on the assumption that one can write a system of partial differential equations in a closed form for the highest derivative in one variable as in equation \eqref{eq:normal_form}. For a first order quasi-linear system in independent variables $t$ and $x$
\begin{equation}
  \label{eq:1st_order_quasilinear_syst}
  \Absf^t(x,t)\drond{\vect{\Uc}}{t} + \Absf^x(x,t) \drond{\vect{\Uc}}{x} + \vect{\Bc} = \vect{0}
\end{equation}
such an expression is possible at a given point ($t_0,x_0$) only if one of the matrices $\Absf^t$ or $\Absf^x$ is non-singular. If for instance the matrix $\Absf^x(t_0,x_0)$ is singular, the line $x=x_0$ is called a \textit{characteristic line} at the point ($t_0,x_0$) otherwise it is a \textit{free} or \textit{non-characteristic} one.

$\Cscr$
$\newline$
We now restrict the problem to the case of \textbf{linear systems} in one space variable in order to introduce the notions of \textit{hyperbolicity} and \textit{characteristics}, system \eqref{eq:general_pde_matrix} hence reads:
\begin{equation}
  \label{eq:1d_pde_matrix}
  \vect{\Uc}_t + \tens{\Asf}\vect{\Uc}_x + \vect{\Bc} = \vect{0} 
\end{equation}
Let us consider the Cauchy initial value problem consisting in finding $\vect{\Uc}_x$ for given initial values of $\vect{\Uc}$ (\textit{i.e. $\vect{\Uc}(x,t=0)$ is known}) on a parametrized curve $\Cc$ of the $(x,t)$ plane, namely by the function $\phi(x,t)=0$, such that equation \eqref{eq:1d_pde_matrix} is satisfied on $\Cc$. The curve $\Cc$ is assumed to satisfy the following regularity requirement $\norm{\nablav \phi} \ne 0$, where $\nablav (\bullet)$ denotes the gradient operator. Since the unknown vector is viewed as a function of $\phi(x,t)$, the following property holds (also assume $\phi_x\ne 0$):
\begin{equation}
  \label{eq:interior_derivative}
  \vect{\Uc}_x\phi_t - \vect{\Uc}_t\phi_x= \vect{0}
\end{equation}

\paragraph{Proof :} If we write the unknown vector as $\vect{\Uc}=\vect{\Wc}(\phi(x,t))$, then by using the chain rule the partial derivatives of $\vect{\Uc}$ read:
\begin{equation*}
  \vect{\Uc}_t = \vect{\Wc}' \: \phi_t \quad ; \quad \vect{\Uc}_x = \vect{\Wc}' \: \phi_x
\end{equation*}
Elimination of $\vect{\Wc}'$ then leads to:
\begin{equation*}
  \vect{\Uc}_t = \vect{\Uc}_x \frac{\phi_t}{\phi_x} 
\end{equation*}
which can be rewritten:
\begin{equation*}
  \vect{\Uc}_t \phi_x = \vect{\Uc}_x \phi_t
\end{equation*}
This ends the proof.

Equation \eqref{eq:interior_derivative} defines a relation between $\vect{\Uc}_t$ and $\vect{\Uc}_x$ that allows to write the original system \eqref{eq:1d_pde_matrix} with $\vect{\Uc}_x$ only:
\begin{equation}
  \label{eq:cauchy_IVP_for_ux}
  \( \lambda\tens{\Isf} + \tens{\Asf} \) \vect{\Uc}_x + \vect{\Bc} = \vect{0} 
\end{equation}
where $\lambda = \phi_t/\phi_x$ and $\tens{\Isf}$ is the identity matrix. It is then obvious that equation \eqref{eq:cauchy_IVP_for_ux} admits a unique solution if and only if the determinant of the system is non-zero, namely:
\begin{equation}
  \label{eq:characteristic_determinant}
  D=\abs{\lambda\tens{\Isf} + \tens{\Asf}} \ne 0
\end{equation}
where D is called the \textit{characteristic determinant} of system \eqref{eq:1d_pde_matrix}. \cite[Page~172,Page~77]{Courant}
% The complete theory of partial differential equations can be found in \cite{Courant}. In what fillows, we will restrict our attention to first order partial differential equations. 
% Suppose we are interested in describing the variation of a physical quantity depending on two independant variables which can be either space or time variables $u(x,y)$.
% \cite[Chapter~3]{Courant};\cite[Chapter~5]{Courant}
% General form of partial differential equations:
% \begin{equation}
%   \label{eq:general_pde}
%   F\(x,y,...,u,u_x,u_y,...,u_{xx},u_{xy},u_{yy},... \) = 0
% \end{equation}
% in which $F$ denotes a combination of partial derivatives of $u$ up to a given order $n$, and the subscript on $u$ stands for partial differentiation $u_x=\drond{u}{x}$. The partial differential equation \eqref{eq:general_pde} is said to be of order $n$ if the highest derivative order is of order $n$. If the function $F$ depends linearly on its variables, the partial differential equation is said to be \textit{linear}. On the other hand, if the funcion $F$ depends linearly on the highest derivatives of $u$ only then the differential equation is \textit{quasi-linear}.

% Definitions: Quasi-linear or Linear equations, order.
% The same definitions holds if the functions $F$ and $u$ are vectors:
% \begin{equation}
%   \label{eq:general_pde_system}
%   \vect{F}\(x,y,...,\vect{u},\vect{u}_x,\vect{u}_y,...,\vect{u}_{xx},\vect{u}_{xy},\vect{u}_{yy},... \) = 0
% \end{equation}
% In matrix form:
% \begin{equation}
%   \label{eq:matrix_pde_system}
%   \tens{A}^x \vect{u}_x + \tens{A}^y \vect{u}_y + \vect{b}= \vect{0}
% \end{equation}
% where the $m \times m$ matrices $\tens{A}^i$ may depend on $\vect{u}$ (quasi-linear).

% In what follows we will restrict to first order partial differential equations. Equation \eqref{eq:general_pde} may be \textit{semilinear} or \textit{linear} depending on whether the right-hand side depends on the unknown functions or not.
% Linear,quasi linear, semi linear 
% General pde form - order ?- linear quasi linear - interior differentiation




%p.172 pour les sytèmes linéaires d'EPD elliptiques et hyperboliques à deux variables indépendantes. (p.173 pour n variables)
%\cite{Toro},\cite{Leveque} pour la forme générale des systèmes hyperboliques du premier ordre. Voir \cite{Courant} pour l'étude des problèmes à plus de deux variables indépendantes qui pourrait justifier qu'on se ramène systématiquement à un problème à deux variables indépendantes pour l'analyse caractéristique.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../mainManuscript"
%%% End:
